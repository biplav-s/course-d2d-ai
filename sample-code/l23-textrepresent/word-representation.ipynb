{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Word Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on material at:\n",
    "# - https://github.com/biplav-s/course-nl/blob/master/l6-probabilistic/Explorations%20in%20Vector%20Representation.ipynb\n",
    "# - https://github.com/biplav-s/course-nl/blob/master/l7-language/code/Context%20with%20TF-IDF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that will encode a-z, space and '.'\n",
    "def do_integer_encoding(data):\n",
    "    # define universe of possible input values\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz .'\n",
    "    \n",
    "    # define a mapping of chars to integers\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    # integer encoded input data\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    char_decoded = [int_to_char[integ] for integ in integer_encoded]\n",
    "    \n",
    "    return integer_encoded, char_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data = hello world, enc = [7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]\n",
      "decoded data: dec = hello world\n"
     ]
    }
   ],
   "source": [
    "# See with a sample string\n",
    "small_data = \"hello world\"\n",
    "enc, dec = do_integer_encoding(small_data)\n",
    "print (\"data = \" + small_data + \", enc = \" + str(enc))\n",
    "print (\"decoded data: \" +  \"dec = \" + \"\".join(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-027f0d56390c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# See with a sample string with out of scope alphabets. Gives error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msmall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hello world 12\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_integer_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"data = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmall_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", enc = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4799cbaf8e54>\u001b[0m in \u001b[0;36mdo_integer_encoding\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# integer encoded input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minteger_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mchar_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minteg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minteger_encoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4799cbaf8e54>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# integer encoded input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minteger_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mchar_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minteg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minteger_encoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "# See with a sample string with out of scope alphabets. Gives error.\n",
    "small_data = \"hello world 12\"\n",
    "enc = do_integer_encoding(small_data)\n",
    "print (\"data = \" + small_data + \", enc = \" + str(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data = This is an important document. It contains the contract governing your deposit relationship with the Bank and required legal disclosures. Please have it translated. \n",
      " enc = [19, 7, 8, 18, 26, 8, 18, 26, 0, 13, 26, 8, 12, 15, 14, 17, 19, 0, 13, 19, 26, 3, 14, 2, 20, 12, 4, 13, 19, 27, 26, 8, 19, 26, 2, 14, 13, 19, 0, 8, 13, 18, 26, 19, 7, 4, 26, 2, 14, 13, 19, 17, 0, 2, 19, 26, 6, 14, 21, 4, 17, 13, 8, 13, 6, 26, 24, 14, 20, 17, 26, 3, 4, 15, 14, 18, 8, 19, 26, 17, 4, 11, 0, 19, 8, 14, 13, 18, 7, 8, 15, 26, 22, 8, 19, 7, 26, 19, 7, 4, 26, 1, 0, 13, 10, 26, 0, 13, 3, 26, 17, 4, 16, 20, 8, 17, 4, 3, 26, 11, 4, 6, 0, 11, 26, 3, 8, 18, 2, 11, 14, 18, 20, 17, 4, 18, 27, 26, 15, 11, 4, 0, 18, 4, 26, 7, 0, 21, 4, 26, 8, 19, 26, 19, 17, 0, 13, 18, 11, 0, 19, 4, 3, 27, 26]\n",
      "decoded data: dec = this is an important document. it contains the contract governing your deposit relationship with the bank and required legal disclosures. please have it translated. \n"
     ]
    }
   ],
   "source": [
    "# Will loose information if we force with out-of-scope alphabets.\n",
    "# Example - lowecases and uppercase\n",
    "medium_data = \"This is an important document. It contains the contract governing \\\n",
    "your deposit relationship with the Bank and required legal \\\n",
    "disclosures. Please have it translated. \"\n",
    "\n",
    "enc, dec = do_integer_encoding(medium_data.lower())\n",
    "\n",
    "print (\"data = \" + medium_data + \"\\n enc = \" + str(enc))\n",
    "print (\"decoded data: \" +  \"dec = \" + \"\".join(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit has support for label encoding\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# But need an array of words as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use spacy to tokenize\n",
    "# Import more\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This' 'is' 'an' 'important' 'document' '.' 'It' 'contains' 'the'\n",
      " 'contract' 'governing' 'your' 'deposit' 'relationship' 'with' 'the'\n",
      " 'Bank' 'and' 'required' 'legal' 'disclosures' '.' 'Please' 'have' 'it'\n",
      " 'translated' '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize larger data for ease\n",
    "\n",
    "doc = nlp(medium_data)\n",
    "values = np.array([token.text for token in doc])\n",
    "print (values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 15  5 14 11  0  2  7 20  8 12 23  9 18 22 20  1  6 19 17 10  0  3 13\n",
      " 16 21  0]\n"
     ]
    }
   ],
   "source": [
    "# Now we can ask for encoding\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This' 'is' 'an' 'important' 'document' '.' 'It' 'contains' 'the'\n",
      " 'contract' 'governing' 'your' 'deposit' 'relationship' 'with' 'the'\n",
      " 'Bank' 'and' 'required' 'legal' 'disclosures' '.' 'Please' 'have' 'it'\n",
      " 'translated' '.']\n"
     ]
    }
   ],
   "source": [
    "# And decode\n",
    "inverted = label_encoder.inverse_transform(integer_encoded)\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an important document . It contains the contract governing your deposit relationship with the Bank and required legal disclosures . Please have it translated .\n"
     ]
    }
   ],
   "source": [
    "# And recreating original string is trivial\n",
    "print (\" \".join(inverted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on sci-kit documentation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "'An alpha document.',\n",
    "'A beta document.',\n",
    "'Guten Morgen!',\n",
    "'Gamma manuscript is old.',\n",
    "'Whither my document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 'an', 'beta', 'document', 'gamma', 'guten', 'is', 'manuscript', 'morgen', 'my', 'old', 'whither']\n",
      "[[1 1 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Single word representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())\n",
    "\n",
    "# Notice document has one dimension although 3 occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha document', 'an alpha', 'an alpha document', 'beta document', 'gamma manuscript', 'gamma manuscript is', 'guten morgen', 'is old', 'manuscript is', 'manuscript is old', 'my document', 'whither my', 'whither my document']\n",
      "[[1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# N-gram representation (2- and 3-; word based)\n",
    "\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 3))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a', ' b', ' d', ' i', ' m', ' o', 'a ', 'al', 'am', 'an', 'be', 'cr', 'cu', 'd.', 'do', 'en', 'er', 'et', 'ga', 'ge', 'gu', 'ha', 'he', 'hi', 'ip', 'is', 'it', 'ld', 'lp', 'ma', 'me', 'mm', 'mo', 'my', 'n ', 'n!', 'nt', 'nu', 'oc', 'ol', 'or', 'ph', 'pt', 'r ', 'rg', 'ri', 's ', 'sc', 't ', 't.', 't?', 'ta', 'te', 'th', 'um', 'us', 'ut', 'wh', 'y ']\n",
      "[[1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      "  1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 2 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1\n",
      "  0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 2 0 1 0 0 0 0\n",
      "  0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0\n",
      "  1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# N-gram representation (2- and 3-; char based)\n",
    "vectorizer3 = CountVectorizer(analyzer='char', ngram_range=(2,2))\n",
    "X3 = vectorizer3.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer3.get_feature_names())\n",
    "print(X3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Representation Using IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 'an', 'beta', 'document', 'gamma', 'guten', 'is', 'manuscript', 'morgen', 'my', 'old', 'whither']\n",
      "[[0.63907044 0.63907044 0.         0.42799292 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.83088075 0.55645052 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.70710678\n",
      "  0.         0.         0.70710678 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.5        0.5        0.         0.         0.5        0.        ]\n",
      " [0.         0.         0.         0.42799292 0.         0.\n",
      "  0.         0.         0.         0.63907044 0.         0.63907044]]\n"
     ]
    }
   ],
   "source": [
    "# TFIDR Vectorizer gives value based on Inverse Document Frequency, i.e., relative\n",
    "# occurence of words in the documents. Hence, context is by word frequency.\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use relative word occurence (similarity) to measure similarity between documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity of doc-1 (An alpha document.) with doc-2 (A beta document.) is = [[0.23815688]]\n",
      "similarity of doc-1 (An alpha document.) with doc-3 (Guten Morgen!) is = [[0.]]\n",
      "similarity of doc-1 (An alpha document.) with doc-4 (Gamma manuscript is old.) is = [[0.]]\n",
      "similarity of doc-1 (An alpha document.) with doc-5 (Whither my document?) is = [[0.18317794]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(corpus)):\n",
    "    print (\"similarity of doc-1 (\" + \n",
    "           corpus[0] + \") with doc-\" + \n",
    "           str(i+1) + \" (\" + corpus[i] + \") is = \"  + \n",
    "           str(cosine_similarity (X[0], X[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
